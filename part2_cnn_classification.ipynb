{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1937b8",
   "metadata": {},
   "source": [
    "# Part 2: CNN Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this part, you'll implement a Convolutional Neural Network (CNN) for EMNIST character recognition. You can choose between TensorFlow/Keras or PyTorch for implementation. This will help you understand CNNs and their advantages for image classification tasks.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Implement a CNN using either TensorFlow/Keras or PyTorch\n",
    "- Apply convolutional layers, pooling, and batch normalization\n",
    "- Train and evaluate the model\n",
    "- Save model and metrics in the correct format\n",
    "\n",
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf813cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup and Installation\n",
    "# Install required packages\n",
    "# %pip install -r requirements.txt\n",
    "\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import string \n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "plt.style.use('seaborn-v0_8-dark-palette')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results/part_2', exist_ok=True)\n",
    "os.makedirs('logs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157aa377",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5b00f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Training data shape: (88800, 28, 28, 1)\n",
      "NumPy Test data shape: (14800, 28, 28, 1)\n",
      "Number of classes: 26\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load EMNIST dataset using TensorFlow Datasets\n",
    "ds_train, ds_test = tfds.load('emnist/letters', split=['train', 'test'], as_supervised=True)\n",
    "\n",
    "# Convert tf.data.Dataset to NumPy arrays\n",
    "def convert_to_numpy(ds):\n",
    "    x = []\n",
    "    y = []\n",
    "    for image, label in tfds.as_numpy(ds):\n",
    "        x.append(image)\n",
    "        y.append(label)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x, y\n",
    "\n",
    "x_train_np, y_train_np = convert_to_numpy(ds_train)\n",
    "x_test_np, y_test_np = convert_to_numpy(ds_test)\n",
    "\n",
    "# Print basic dataset information (NumPy shapes)\n",
    "print(f\"NumPy Training data shape: {x_train_np.shape}\")\n",
    "print(f\"NumPy Test data shape: {x_test_np.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train_np))}\")\n",
    "\n",
    "# Plot sample images (using NumPy arrays)\n",
    "alphabet = string.ascii_uppercase + string.ascii_lowercase\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(x_train_np[i].squeeze().T, cmap='gray')\n",
    "    plt.title(f'Label: {alphabet[y_train_np[i] - 1]}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "703480de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch DataLoaders created.\n",
      "PyTorch Training data shape for loader: torch.Size([32, 1, 28, 28])\n",
      "PyTorch Validation data shape for loader: torch.Size([32, 1, 28, 28])\n",
      "PyTorch Test data shape for loader: torch.Size([32, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np # Ensure numpy is imported here\n",
    "\n",
    "# Preprocessing for PyTorch\n",
    "x_train_np_squeezed = x_train_np.squeeze(-1) # Remove the last dimension if it's 1\n",
    "x_test_np_squeezed = x_test_np.squeeze(-1)\n",
    "\n",
    "x_train_pt = torch.tensor(x_train_np_squeezed, dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "x_test_pt = torch.tensor(x_test_np_squeezed, dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "y_train_pt = torch.tensor(y_train_np - 1, dtype=torch.long)\n",
    "y_test_pt = torch.tensor(y_test_np - 1, dtype=torch.long)\n",
    "\n",
    "# Split into train and validation sets\n",
    "x_train_pt, x_val_pt, y_train_pt, y_val_pt = train_test_split(\n",
    "    x_train_pt, y_train_pt, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create TensorDatasets and DataLoaders\n",
    "train_dataset = TensorDataset(x_train_pt, y_train_pt)\n",
    "val_dataset = TensorDataset(x_val_pt, y_val_pt)\n",
    "test_dataset = TensorDataset(x_test_pt, y_test_pt)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "print(\"PyTorch DataLoaders created.\")\n",
    "print(f\"PyTorch Training data shape for loader: {next(iter(train_loader))[0].shape}\")\n",
    "print(f\"PyTorch Validation data shape for loader: {next(iter(val_loader))[0].shape}\")\n",
    "print(f\"PyTorch Test data shape for loader: {next(iter(test_loader))[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee97ebf",
   "metadata": {},
   "source": [
    "## 2. Model Implementation\n",
    "\n",
    "### TensorFlow/Keras Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97872be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6272</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">802,944</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,354</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6272\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m802,944\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m)             │         \u001b[38;5;34m3,354\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">899,866</span> (3.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m899,866\u001b[0m (3.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">899,418</span> (3.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m899,418\u001b[0m (3.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create CNN using Keras\n",
    "def create_cnn_keras(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create a CNN using TensorFlow/Keras.\n",
    "\n",
    "    Requirements:\n",
    "    - Must use at least 2 convolutional layers\n",
    "    - Must include pooling and batch normalization\n",
    "    - Must use categorical crossentropy loss\n",
    "\n",
    "    Goals:\n",
    "    - Achieve > 85% accuracy on test set\n",
    "    - Minimize overfitting using batch normalization and dropout\n",
    "    - Train efficiently with appropriate batch size and learning rate\n",
    "\n",
    "    Args:\n",
    "        input_shape: Shape of input data (should be (28, 28, 1) for grayscale images)\n",
    "        num_classes: Number of output classes (26 for letters)\n",
    "\n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "\n",
    "        # First convolutional block\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        # Second convolutional block\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        # Optional third conv layer (improves performance)\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "        # Flatten and dense layers\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create and compile model\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes = 26\n",
    "keras_cnn_model = create_cnn_keras(input_shape, num_classes)\n",
    "keras_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2fb286",
   "metadata": {},
   "source": [
    "### PyTorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7dc56bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout2): Dropout(p=0.25, inplace=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=6272, out_features=128, bias=True)\n",
      "  (dropout_fc): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=26, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        \"\"\"\n",
    "        Create a CNN using PyTorch.\n",
    "\n",
    "        Requirements:\n",
    "        - Must use at least 2 convolutional layers\n",
    "        - Must include pooling and batch normalization\n",
    "\n",
    "        Goals:\n",
    "        - Achieve > 85% accuracy on test set\n",
    "        - Minimize overfitting using batch normalization and dropout\n",
    "        - Train efficiently with appropriate batch size and learning rate\n",
    "\n",
    "        Args:\n",
    "            num_classes: Number of output classes (26 for letters)\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "\n",
    "        # Optional third convolutional block (can improve performance)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 128) # Assuming 28x28 input\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, channels, height, width)\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # First convolutional block\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Second convolutional block\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Optional third convolutional block\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "\n",
    "        # Flatten for fully connected layers\n",
    "        x = torch.flatten(x, 1) # More robust way to flatten\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "pytorch_cnn_model = CNN(num_classes=26)\n",
    "print(pytorch_cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90ea644",
   "metadata": {},
   "source": [
    "## 3. Training and Evaluation\n",
    "\n",
    "### TensorFlow/Keras Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f84596",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TensorFlow/Keras Training\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'models/cnn_keras.keras',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train model\n",
    "history_keras = keras_cnn_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "ax1.plot(history_keras.history['accuracy'], label='Training')\n",
    "ax1.plot(history_keras.history['val_accuracy'], label='Validation')\n",
    "ax1.set_title('Keras Model Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot loss\n",
    "ax2.plot(history_keras.history['loss'], label='Training')\n",
    "ax2.plot(history_keras.history['val_loss'], label='Validation')\n",
    "ax2.set_title('Keras Model Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss_keras, accuracy_keras = keras_cnn_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Keras Test Loss: {loss_keras:.4f}')\n",
    "print(f'Keras Test Accuracy: {accuracy_keras:.4f}')\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_keras = keras_cnn_model.predict(x_test)\n",
    "y_pred_classes_keras = np.argmax(y_pred_keras, axis=1)\n",
    "y_true_classes_keras = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate and print metrics\n",
    "print(\"\\nKeras Classification Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_true_classes_keras, y_pred_classes_keras):.4f}\")\n",
    "print(f\"Precision (macro): {precision_score(y_true_classes_keras, y_pred_classes_keras, average='macro'):.4f}\")\n",
    "print(f\"Recall (macro): {recall_score(y_true_classes_keras, y_pred_classes_keras, average='macro'):.4f}\")\n",
    "print(f\"F1-score (macro): {f1_score(y_true_classes_keras, y_pred_classes_keras, average='macro'):.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm_keras = confusion_matrix(y_true_classes_keras, y_pred_classes_keras)\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(cm_keras, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=alphabet, yticklabels=alphabet)\n",
    "plt.title('Keras Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Save model and metrics\n",
    "keras_metrics = {\n",
    "    'test_loss': loss_keras,\n",
    "    'test_accuracy': accuracy_keras,\n",
    "    'accuracy': accuracy_score(y_true_classes_keras, y_pred_classes_keras),\n",
    "    'precision_macro': precision_score(y_true_classes_keras, y_pred_classes_keras, average='macro'),\n",
    "    'recall_macro': recall_score(y_true_classes_keras, y_pred_classes_keras, average='macro'),\n",
    "    'f1_macro': f1_score(y_true_classes_keras, y_pred_classes_keras, average='macro')\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('results/part_2/keras_metrics.json', 'w') as f:\n",
    "    json.dump(keras_metrics, f)\n",
    "\n",
    "print(\"\\nKeras model and metrics saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad3536f",
   "metadata": {},
   "source": [
    "### PyTorch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26c81ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 1.1132, Train Acc: 0.6407, Val Loss: 0.3332, Val Acc: 0.8912\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m inputs, labels = inputs.to(device), labels.to(device)\n\u001b[32m     25\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     28\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     61\u001b[39m x = \u001b[38;5;28mself\u001b[39m.conv2(x)\n\u001b[32m     62\u001b[39m x = F.relu(\u001b[38;5;28mself\u001b[39m.bn2(x))\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout2(x)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Optional third convolutional block\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/pooling.py:213\u001b[39m, in \u001b[36mMaxPool2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/_jit_internal.py:624\u001b[39m, in \u001b[36mboolean_dispatch.<locals>.fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(*args, **kwargs)\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/functional.py:830\u001b[39m, in \u001b[36m_max_pool2d\u001b[39m\u001b[34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[39m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    829\u001b[39m     stride = torch.jit.annotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[32m--> \u001b[39m\u001b[32m830\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = pytorch_cnn_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "history_pytorch = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "best_val_loss_pytorch = float('inf')\n",
    "patience_pytorch = 5\n",
    "patience_counter_pytorch = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = train_correct / train_total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    # Save history\n",
    "    history_pytorch['train_loss'].append(train_loss)\n",
    "    history_pytorch['train_acc'].append(train_acc)\n",
    "    history_pytorch['val_loss'].append(val_loss)\n",
    "    history_pytorch['val_acc'].append(val_acc)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss_pytorch:\n",
    "        best_val_loss_pytorch = val_loss\n",
    "        patience_counter_pytorch = 0\n",
    "        torch.save(model.state_dict(), 'models/cnn_pytorch.pt')\n",
    "    else:\n",
    "        patience_counter_pytorch += 1\n",
    "        if patience_counter_pytorch >= patience_pytorch:\n",
    "            print(\"PyTorch Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "ax1.plot(history_pytorch['train_acc'], label='Training')\n",
    "ax1.plot(history_pytorch['val_acc'], label='Validation')\n",
    "ax1.set_title('PyTorch Model Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot loss\n",
    "ax2.plot(history_pytorch['train_loss'], label='Training')\n",
    "ax2.plot(history_pytorch['val_loss'], label='Validation')\n",
    "ax2.set_title('PyTorch Model Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "test_acc = test_correct / test_total\n",
    "print(f'PyTorch Test Loss: {test_loss:.4f}')\n",
    "print(f'PyTorch Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Calculate and print metrics\n",
    "print(\"\\nPyTorch Classification Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "print(f\"Precision (macro): {precision_score(all_labels, all_preds, average='macro'):.4f}\")\n",
    "print(f\"Recall (macro): {recall_score(all_labels, all_preds, average='macro'):.4f}\")\n",
    "print(f\"F1-score (macro): {f1_score(all_labels, all_preds, average='macro'):.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm_pytorch = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(cm_pytorch, annot=True, fmt='d', cmap='viridis',\n",
    "            xticklabels=alphabet, yticklabels=alphabet)\n",
    "plt.title('PyTorch Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Save model and metrics\n",
    "pytorch_metrics = {\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'accuracy': accuracy_score(all_labels, all_preds),\n",
    "    'precision_macro': precision_score(all_labels, all_preds, average='macro'),\n",
    "    'recall_macro': recall_score(all_labels, all_preds, average='macro'),\n",
    "    'f1_macro': f1_score(all_labels, all_preds, average='macro')\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('results/part_2/pytorch_metrics.json', 'w') as f:\n",
    "    json.dump(pytorch_metrics, f)\n",
    "\n",
    "print(\"\\nPyTorch model and metrics saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4775fc2d",
   "metadata": {},
   "source": [
    "## Progress Checkpoints\n",
    "\n",
    "1. **Data Loading**:\n",
    "   - [ ] Successfully load EMNIST dataset\n",
    "   - [ ] Verify data shapes and ranges\n",
    "   - [ ] Visualize sample images\n",
    "\n",
    "2. **Preprocessing**:\n",
    "   - [ ] Normalize pixel values\n",
    "   - [ ] Reshape data for CNN input\n",
    "   - [ ] Convert labels to one-hot encoding\n",
    "\n",
    "3. **Model Implementation**:\n",
    "   - [ ] Create CNN with required layers\n",
    "   - [ ] Verify architecture requirements\n",
    "   - [ ] Test model with sample input\n",
    "\n",
    "4. **Training**:\n",
    "   - [ ] Train model with callbacks\n",
    "   - [ ] Monitor training progress\n",
    "   - [ ] Save best model\n",
    "\n",
    "5. **Evaluation**:\n",
    "   - [ ] Calculate performance metrics\n",
    "   - [ ] Save metrics in correct format\n",
    "   - [ ] Visualize results\n",
    "\n",
    "## Common Issues and Solutions\n",
    "\n",
    "1. **Data Loading Issues**:\n",
    "   - Problem: EMNIST dataset not found\n",
    "   - Solution: Check internet connection and TensorFlow installation\n",
    "\n",
    "2. **Preprocessing Issues**:\n",
    "   - Problem: Shape mismatch in CNN layers\n",
    "   - Solution: Ensure data is properly shaped (samples, height, width, channels)\n",
    "   - Problem: Label encoding errors\n",
    "   - Solution: Verify label range and one-hot encoding\n",
    "\n",
    "3. **Model Issues**:\n",
    "   - Problem: Training instability\n",
    "   - Solution: Add batch normalization, reduce learning rate\n",
    "   - Problem: Overfitting\n",
    "   - Solution: Increase dropout, use data augmentation\n",
    "\n",
    "4. **Evaluation Issues**:\n",
    "   - Problem: Metrics format incorrect\n",
    "   - Solution: Follow the exact format specified\n",
    "   - Problem: Performance below threshold\n",
    "   - Solution: Adjust architecture, hyperparameters"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
